{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Lab\n",
    "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine. This lab presents how to work with Spark SQL.\n",
    "\n",
    "### Creating the DataFrame\n",
    "The `SparkSession` class is the entry point for the DataFrames API. This class exposes a `DataFrameReader` named `read` that can be used to create a DataFrame from existing data in supported formats. In our application, we create a `SparkSession` and then create a DataFrame from a JSON file. The dataset we are using in this lab is the results of the March 2016 Virginia Primary Elections for Presidency. The file, `loudoun_d_primary_results_2016.json`, located in `data/sql`, in which each line has the following structure:\n",
    "```\n",
    "{\n",
    "  \"district_type\": \"Congressional\", \n",
    "  \"last_name\": \"Clinton\", \n",
    "  \"candidate_ballot_order\": \"1\", \n",
    "  \"precinct_code\": \"###PROV\", \n",
    "  \"referendumId\": \"\", \n",
    "  \"total_votes\": \"9\", \n",
    "  \"candidate_name\": \"Hillary Clinton\", \n",
    "  \"locality_name\": \"LOUDOUN COUNTY\", \n",
    "  \"office_ballot_order\": \"1\", \n",
    "  \"party\": \"Democratic\", \n",
    "  \"election_name\": \"2016 March Democratic Presidential Primary\", \n",
    "  \"election_date\": \"2016-03-01 00:00:00.000\", \n",
    "  \"precinct_name\": \"## Provisional\", \n",
    "  \"null\": [\"\"], \n",
    "  \"locality_code\": \"107\",\n",
    "  \"negative_votes\": \"\",\n",
    "  \"office_name\": \"President\",\n",
    "  \"candidateId\": \"124209128\",\n",
    "  \"DESCRIPTION\": \"10th District\",\n",
    "  \"districtId\": \"1085224094\",\n",
    "  \"referendum_title\": \"\",\n",
    "  \"officeId\": \"933838092\",\n",
    "  \"in_precinct\": \"## Provisional\",\n",
    "  \"election_type\": \"Primary\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@fa0273f\n",
       "fileName = data/sql/loudoun_d_primary_results_2016.json\n",
       "df = [DESCRIPTION: string, candidateId: string ... 22 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DESCRIPTION: string, candidateId: string ... 22 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkSQL\").getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "// Create a DataFrame based on the JSON results.\n",
    "val fileName = \"data/sql/loudoun_d_primary_results_2016.json\"\n",
    "val df = spark.read.json(fileName)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print the inferred schema of the data, and the first 2 lines of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DESCRIPTION: string (nullable = true)\n",
      " |-- candidateId: string (nullable = true)\n",
      " |-- candidate_ballot_order: string (nullable = true)\n",
      " |-- candidate_name: string (nullable = true)\n",
      " |-- districtId: string (nullable = true)\n",
      " |-- district_type: string (nullable = true)\n",
      " |-- election_date: string (nullable = true)\n",
      " |-- election_name: string (nullable = true)\n",
      " |-- election_type: string (nullable = true)\n",
      " |-- in_precinct: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- locality_code: string (nullable = true)\n",
      " |-- locality_name: string (nullable = true)\n",
      " |-- negative_votes: string (nullable = true)\n",
      " |-- null: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- officeId: string (nullable = true)\n",
      " |-- office_ballot_order: string (nullable = true)\n",
      " |-- office_name: string (nullable = true)\n",
      " |-- party: string (nullable = true)\n",
      " |-- precinct_code: string (nullable = true)\n",
      " |-- precinct_name: string (nullable = true)\n",
      " |-- referendumId: string (nullable = true)\n",
      " |-- referendum_title: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([10th District,124209128,1,Hillary Clinton,1085224094,Congressional,2016-03-01 00:00:00.000,2016 March Democratic Presidential Primary,Primary,## Provisional,Clinton,107,LOUDOUN COUNTY,,WrappedArray(),933838092,1,President,Democratic,###PROV,## Provisional,,,9], [10th District,1999936198,2,Martin J. O'Malley,1085224094,Congressional,2016-03-01 00:00:00.000,2016 March Democratic Presidential Primary,Primary,## Provisional,O'Malley,107,LOUDOUN COUNTY,,WrappedArray(),933838092,1,President,Democratic,###PROV,## Provisional,,,0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "// print the dataframe schema\n",
    "df.printSchema()\n",
    "\n",
    "// print the first two rows\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming and Querying the DataFrame\n",
    "Let's explore the data to determine who the candidates on the ballot were, based on the unique names in the `candidate_name` field. You should receive the following result:\n",
    "```\n",
    "+------------------+\n",
    "|    candidate_name|\n",
    "+------------------+\n",
    "|    Bernie Sanders|\n",
    "|   Hillary Clinton|\n",
    "|Martin J. O'Malley|\n",
    "+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    candidate_name|\n",
      "+------------------+\n",
      "|    Bernie Sanders|\n",
      "|   Hillary Clinton|\n",
      "|Martin J. O'Malley|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "// get all distinct candidate names from the DataFrame\n",
    "df.select(\"candidate_name\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what order the candidates were printed on the ballots, using the `candidate_ballot_order` field. In Virginia, every county uses the same ballot, so we only need one sampling and can safely discard the duplicates. Since we are going to reuse the following DataFrame, use the `cache()` method to cache it. Here is the result:\n",
    "```\n",
    "+------------------+----------------------+\n",
    "|    candidate_name|candidate_ballot_order|\n",
    "+------------------+----------------------+\n",
    "|   Hillary Clinton|                     1|\n",
    "|Martin J. O'Malley|                     2|\n",
    "|    Bernie Sanders|                     3|\n",
    "+------------------+----------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+\n",
      "|    candidate_name|candidate_ballot_order|\n",
      "+------------------+----------------------+\n",
      "|   Hillary Clinton|                     1|\n",
      "|Martin J. O'Malley|                     2|\n",
      "|    Bernie Sanders|                     3|\n",
      "+------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orderDF = [candidate_name: string, candidate_ballot_order: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[candidate_name: string, candidate_ballot_order: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "// get the ballot order and discard the many duplicates (all VA ballots are the same)\n",
    "val orderDF = df.select(df(\"candidate_name\"), df(\"candidate_ballot_order\")).distinct().orderBy(\"candidate_ballot_order\").cache()\n",
    "orderDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above query that showed the ballot order needs to be changed to show descriptive English text instead of numbers. We have a reference lookup table available in the file called `friendly_orders.json` in `data/sql` that we would like to use. This file has the following structure:\n",
    "```\n",
    "{\"candidate_ballot_order\": \"1\", \"friendly_name\": \"First on Ballot\"}\n",
    "{\"candidate_ballot_order\": \"2\", \"friendly_name\": \"In Middle of Ballot\"}\n",
    "{\"candidate_ballot_order\": \"3\", \"friendly_name\": \"Last on Ballot\"}\n",
    "```\n",
    "\n",
    "We create a DataFrame of this reference data and then use it to alter the output of our ballot order query, and show the `friendly_name` instead of numbers. You should get a result as below:\n",
    "```\n",
    "+------------------+-------------------+\n",
    "|    candidate_name|      friendly_name|\n",
    "+------------------+-------------------+\n",
    "|   Hillary Clinton|    First on Ballot|\n",
    "|Martin J. O'Malley|In Middle of Ballot|\n",
    "|    Bernie Sanders|     Last on Ballot|\n",
    "+------------------+-------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|    candidate_name|      friendly_name|\n",
      "+------------------+-------------------+\n",
      "|   Hillary Clinton|    First on Ballot|\n",
      "|Martin J. O'Malley|In Middle of Ballot|\n",
      "|    Bernie Sanders|     Last on Ballot|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[candidate_name: string, candidate_ballot_order: string ... 2 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "orderFileName = data/sql/friendly_orders.json\n",
       "friendlyDF = [candidate_ballot_order: string, friendly_name: string]\n",
       "joinedDF = [candidate_name: string, candidate_ballot_order: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "val orderFileName = \"data/sql/friendly_orders.json\"\n",
    "val friendlyDF = spark.read.json(orderFileName)\n",
    "\n",
    "// join the tables so the results show descriptive text\n",
    "val joinedDF = orderDF.join(friendlyDF, orderDF(\"candidate_ballot_order\") === friendlyDF(\"candidate_ballot_order\"), \"inner\")\n",
    "\n",
    "// hide the numeric column in the output.\n",
    "joinedDF.select(joinedDF(\"candidate_name\"), joinedDF(\"friendly_name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try an aggregate query. To count the total votes, we must cast the `total_votes` column to numeric data and then take the sum of every cell. Let's assign an alias to the column after the cast, using the `alias` method, to increase readability. Here is the result:\n",
    "```\n",
    "+--------------------+\n",
    "|sum(total_votes_int)|\n",
    "+--------------------+\n",
    "|               36149|\n",
    "+--------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(total_votes_int)|\n",
      "+--------------------+\n",
      "|               36149|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "votesColumn = CAST(total_votes AS INT) AS `total_votes_int`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CAST(total_votes AS INT) AS `total_votes_int`"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "// orginal data is string-based, so create an integer version of it and call it total_votes_int\n",
    "val votesColumn = df(\"total_votes\").cast(\"int\").alias(\"total_votes_int\")\n",
    "\n",
    "// get the integer-based votes column and sum all values together\n",
    "df.select(sum(votesColumn).alias(\"sum(total_votes_int)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping this vote count by `candidate_name` employs a similar pattern. Let's use `orderBy()` to sort the results, and show how many votes each candidate got, as below:\n",
    "```\n",
    "+------------------+----------+\n",
    "|    candidate_name|sum_column|\n",
    "+------------------+----------+\n",
    "|   Hillary Clinton|     21180|\n",
    "|    Bernie Sanders|     14730|\n",
    "|Martin J. O'Malley|       239|\n",
    "+------------------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|    candidate_name|sum_column|\n",
      "+------------------+----------+\n",
      "|   Hillary Clinton|     21180|\n",
      "|    Bernie Sanders|     14730|\n",
      "|Martin J. O'Malley|       239|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "candidateDF = [candidate_name: string, total_votes_int: int]\n",
       "groupedDF = [candidate_name: string, sum_column: bigint]\n",
       "summaryDF = [candidate_name: string, sum_column: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[candidate_name: string, sum_column: bigint]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "// get just the candidate names and votes.\n",
    "val candidateDF = df.select(df(\"candidate_name\"), votesColumn)\n",
    "\n",
    "// group by candidate name and sum votes, and assign an alias \"sum_column\" to the sum so we can order\n",
    "// on that column.\n",
    "val groupedDF = candidateDF.groupBy(\"candidate_name\").agg(sum(\"total_votes_int\").alias(\"sum_column\"))\n",
    "val summaryDF = groupedDF.orderBy(desc(\"sum_column\")).cache()\n",
    "summaryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final exploration, we see which physical precincts (polling station) had the highest physical turnout. Virginia designates special theoretical precincts for absentee and provisional ballots, which can skew our results. So, we want to omit these precincts from our query. A glance at the data shows that the theoretical precincts have non-integer values for `precinct_code`. We can apply cast to the `precinct_code` column and then filter out the rows containing non-integer codes. All physical precincts have a numeric code. Provisional/absentee precincts start with \"##\". We expect to see the result as below:\n",
    "```\n",
    "+-------------+----------+\n",
    "|precinct_name|sum_column|\n",
    "+-------------+----------+\n",
    "| 314 - LEGACY|       652|\n",
    "+-------------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "Can't extract value from cast(precinct_code#25 as int) AS precinct_code_int#418: need struct type but got int;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Can't extract value from cast(precinct_code#25 as int) AS precinct_code_int#418: need struct type but got int;",
      "  at org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:73)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:897)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:898)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:898)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:898)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$35.apply(Analyzer.scala:958)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$35.apply(Analyzer.scala:958)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:958)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:901)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:901)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:758)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)",
      "  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)",
      "  at scala.collection.immutable.List.foldLeft(List.scala:84)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1334)",
      "  ... 46 elided"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "// Spark's cast function converts these to \"null\".\n",
    "val precinctColumn = df(\"precinct_code\").cast(\"int\").alias(\"precinct_code_int\")\n",
    "\n",
    "// get the precinct name, integer-based code, and integer-based votes, then filter on non-null codes.\n",
    "val pollingDF = df.select(df(\"precinct_name\"), precinctColumn(\"precinct_code_int\"), candidateDF(\"total_votes_int\")).filter(precinctColumn(\"precinct_code_int\").isNotNull)\n",
    "\n",
    "// group by precinct name and sum votes, and assign an alias \"sum_column\" to the sum so we can order on that\n",
    "// column, and then, show the max row.\n",
    "val groupedPollDF = pollingDF.groupBy(\"precinct_name\").agg(sum(\"total_votes_int\").alias(\"sum_column\")).orderBy(desc(\"sum_column\"))\n",
    "groupedPollDF.select(max(\"sum_column\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the DataFrame\n",
    "The `DataFrame` class  exposes a `DataFrameWriter` named write that can be used to save a DataFrame. There are four available write modes which can be specified, with error being the default:\n",
    "* `append`: add this data to the end of any data already at the target location.\n",
    "* `overwrite`: erase any existing data at the target location and replace with this data.\n",
    "* `ignore`: silently skip this command if any data already exists at the target location.\n",
    "* `error`: throw an exception if any data already exists at the target location.\n",
    "\n",
    "Here, we save one of our generated DataFrames as JSON data in the folder `target/json` with the `overwrite` mode. If you look in this directory after running the code, you will see a separate JSON file for each row of the DataFrame, along with a `_SUCCESS` indicator file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "summaryDF.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
